<h1 id="ai-">AI μ§λ¬΄ ν‰κ°€ λ€λΉ„ κ°•μ μλ£ (κΈ°μ  λΈ”λ΅κ·Έ μ¤νƒ€μΌ)</h1>
<h2 id="1-">1. λ°μ΄ν„° μ „μ²λ¦¬</h2>
<h3 id="-bias-">λ°μ΄ν„° μμ§‘κ³Ό νΈν–¥ (Bias)</h3>
<ul>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> λ°μ΄ν„° μμ§‘ μ‹ νΈν–¥ μµμ†ν™”κ°€ μ¤‘μ”ν•©λ‹λ‹¤. ν• λ…μ μ „λ¬Έκ°€λ§ λ μ΄λΈ”λ§ν•λ©΄ μΌκ΄€μ„±μ€ μƒκΈ°μ§€λ§ κ°μΈμ  νΈν–¥μ΄ μ „μ²΄ λ°μ΄ν„°μ…‹μ— λ°μλ©λ‹λ‹¤.  </li>
<li><strong>μ‹¤λ¬΄ μ‚¬λ΅€:</strong> ImageNetλ„ μ΄κΈ°μ—λ” μ„κµ¬κ¶ λ°μ΄ν„° μ„μ£ΌλΌμ„ μΈμΆ…Β·λ¬Έν™”μ  νΈν–¥ λ¬Έμ κ°€ λ…Όμλμ—μµλ‹λ‹¤. Googleμ β€Inclusive Images Challenge(2018)β€λ” λ‹¤μ–‘ν• μ§€μ—­ λ°μ΄ν„°λ¥Ό μ¶”κ°€ν•΄ μ΄λ¥Ό μ™„ν™”ν•κ³ μ ν–μµλ‹λ‹¤.  </li>
<li><strong>μ—°κµ¬ μ°Έκ³ :</strong>  </li>
<li>Torralba &amp; Efros, <em>Unbiased Look at Dataset Bias</em> (CVPR 2011).  </li>
<li>Mitchell et al., <em>Model Cards for Model Reporting</em> (FAT* 2019).  <h3 id="-normalization-vs-standardization-">λ°μ΄ν„° μ •μ  (Normalization vs. Standardization)</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong>  </li>
<li>Min-Max Normalization: [0,1] κµ¬κ°„μΌλ΅ μ¤μΌ€μΌλ§ β†’ μ΄μƒμΉ(outlier)μ— λ―Όκ°.  </li>
<li>Standardization: ν‰κ·  0, ν‘μ¤€νΈμ°¨ 1λ΅ λ³€ν™ β†’ νκ·€, PCAμ— μ ν•©.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>LeCun et al. (1998) MNIST β†’ μ…λ ¥ λ°μ΄ν„° μ •κ·ν™”λ¥Ό ν†µν•΄ λΉ λ¥Έ ν•™μµ λ‹¬μ„±.  </li>
<li>Ioffe &amp; Szegedy, <em>Batch Normalization</em> (ICML 2015).  <h3 id="-data-augmentation-">λ°μ΄ν„° μ¦κ°• (Data Augmentation)</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> μμ¨μ£Όν–‰ Semantic Segmentationμ—λ” Random Erasingμ΄ μ ν•©. μ΄λ” κ°€λ ¤μ§ μƒν™©μ„ ν•™μµμ‹μΌ λ¨λΈμ„ λ” κ°•μΈν•κ² λ§λ“­λ‹λ‹¤.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Zhong et al., <em>Random Erasing Data Augmentation</em> (AAAI 2020).  </li>
<li>CutMix, Mixup, AugMix λ“±μ€ λ¶„λ¥ κ³Όμ μ— κ°•μ .  </li>
</ul>
<hr>
<h2 id="2-ai-">2. AI λ¨λΈ κ°λ°</h2>
<h3 id="-self-supervised-learning">μ•„ν‚¤ν…μ² μ„¤κ³„ &amp; Self-Supervised Learning</h3>
<ul>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> SimCLR, BYOL, MAE, RotNet λ“± self-supervised κΈ°λ²•μ ν•µμ‹¬ μ΄ν•΄.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Chen et al., <em>SimCLR</em> (ICML 2020).  </li>
<li>Grill et al., <em>BYOL</em> (NeurIPS 2020).  </li>
<li>He et al., <em>MAE</em> (CVPR 2022).  <h3 id="explainable-ai-xai-">Explainable AI (XAI)</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> CAM vs. Grad-CAM κµ¬λ¶„. CAMμ€ FC layer κ°€μ¤‘μΉ κΈ°λ°, Grad-CAMμ€ μ†μ‹¤ κΈ°μΈκΈ° κΈ°λ°.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Zhou et al., <em>CAM</em> (CVPR 2016).  </li>
<li>Selvaraju et al., <em>Grad-CAM</em> (ICCV 2017).  <h3 id="-learning-curve-">λ¨λΈ ν•™μµ &amp; ν‰κ°€ (Learning Curve, μ§€ν‘)</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong>  </li>
<li>μ–Έλ”ν”Όν…: λ¨λΈ λ³µμ΅λ„ β†‘, νΉμ§• β†‘ ν•„μ”.  </li>
<li>κ³Όμ ν•©: Dropout, L2 μ •κ·ν™”, BatchNorm λ“± ν™μ©.  </li>
<li>μ§€ν‘: Recall = μ•μ „-critical ν™κ²½μ—μ„ μ¤‘μ”.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Srivastava et al., <em>Dropout</em> (JMLR 2014).  </li>
<li>Powers, <em>Evaluation: Precision, Recall, F-measure</em> (2011).  <h3 id="-hpo-class-imbalance-">λ¨λΈ νλ‹ (HPO &amp; Class Imbalance)</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong>  </li>
<li>Bayesian Optimization: ν¨μ¨μ  HPO.  </li>
<li>ν΄λμ¤ λ¶κ· ν• β†’ SMOTE, μ–Έλ”μƒν”λ§, Focal Loss λ“±.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Bergstra &amp; Bengio, <em>Random Search for Hyper-Parameter Optimization</em> (JMLR 2012).  </li>
<li>Lin et al., <em>Focal Loss</em> (ICCV 2017).  </li>
</ul>
<hr>
<h2 id="3-ai-">3. AI μ‹μ¤ν… κµ¬μ¶•</h2>
<h3 id="ml-pipeline-">ML Pipeline &amp; λ°°ν¬ μ „λµ</h3>
<ul>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> Model-in-service vs. Model-as-service λΉ„κµ.  </li>
<li>In-service: κΈ°μ΅΄ μΈν”„λΌ μ¬ν™μ©, μ„λ²„ λ¦¬μ†μ¤ μ μ β†‘.  </li>
<li>As-service: ν™•μ¥μ„±β†‘, λ…λ¦½μ  κ΄€λ¦¬ μ©μ΄.  </li>
<li><strong>μ‚¬λ΅€:</strong> Google TFX, Kubeflow / Docker-K8s κΈ°λ° CI/CD.  <h3 id="mlops-">MLOps &amp; μλ™ν™”</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> MLOps maturity level: μλ™(Level 0) β†” μλ™ν™”(Level 1+).  </li>
<li><strong>μ‚¬λ΅€/λ°±μ„:</strong>  </li>
<li>Google Cloud, <em>Continuous Training for ML</em> (2020).  </li>
<li>AWS Sagemaker, Azure ML docs.  <h3 id="-">λ¨λΈ μµμ ν™”</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> λ¨λΈ κ²½λ‰ν™” κΈ°λ²• β†’ Pruning, Quantization, EfficientNetμ Compound Scaling.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Han et al., <em>Deep Compression</em> (ICLR 2016).  </li>
<li>Tan &amp; Le, <em>EfficientNet</em> (ICML 2019).  </li>
</ul>
<hr>
<h2 id="4-ai-">4. μ£Όμ” AI νΈλ λ“</h2>
<h3 id="zero-shot-generalized-zero-shot-learning">Zero-shot &amp; Generalized Zero-shot Learning</h3>
<ul>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> μƒλ΅μ΄ ν΄λμ¤ μΈμ‹ λ¥λ ¥.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Xian et al., <em>Zero-shot Learning β€“ A Comprehensive Evaluation</em> (TPAMI 2018).  </li>
<li>Radford et al., <em>CLIP</em> (ICML 2021).  <h3 id="chain-of-thought-prompting">Chain-of-Thought Prompting</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> λ‹¨κ³„μ  μ¶”λ΅  μ λ„.  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Wei et al., <em>Chain-of-Thought Prompting</em> (NeurIPS 2022).  <h3 id="nas-darts">NAS &amp; DARTS</h3>
</li>
<li><strong>μ‹ν— ν¬μΈνΈ:</strong> Neural Architecture Search, λ―Έλ¶„ κ°€λ¥ νƒμƒ‰(DARTS).  </li>
<li><strong>μ—°κµ¬/μ‚¬λ΅€:</strong>  </li>
<li>Zoph &amp; Le, <em>Neural Architecture Search</em> (ICLR 2017).  </li>
<li>Liu et al., <em>DARTS</em> (ICLR 2019).  </li>
</ul>
<hr>
<p>π”Ή Level 0: μλ™ μ΄μ (Manual)</p>
<p>λ¨λΈ κ°λ°, λ°°ν¬, μ¬ν•™μµμ„ μ‚¬λμ΄ μ§μ ‘ μ²λ¦¬.</p>
<p>λ°μ΄ν„° μ¤€λΉ„ β†’ ν•™μµ β†’ ν‰κ°€ β†’ λ°°ν¬κ°€ λ¨λ‘ λ‹¨λ°μ„± ν”„λ΅μ νΈ μ„±κ²©.</p>
<p>μ¬ν„μ„± λ¶€μ΅±, μλ™ν™” μ—†μ.</p>
<p>μ¤νƒ€νΈμ—…Β·μ—°κµ¬ ν”„λ΅μ νΈ μ΄κΈ° λ‹¨κ³„μ—μ„ ν”ν λ‚νƒ€λ‚¨.</p>
<p>π”Ή Level 1: μλ™ν™”λ νμ΄ν”„λΌμΈ (ML Pipeline Automation)</p>
<p>λ°μ΄ν„° μ „μ²λ¦¬, ν•™μµ, ν‰κ°€, λ°°ν¬ λ‹¨κ³„λ¥Ό CI/CD νμ΄ν”„λΌμΈμ²λΌ μ—°κ²°.</p>
<p>μ½”λ“ λ³€κ²½ β†’ μλ™ μ¬ν•™μµ λ° λ°°ν¬ κ°€λ¥.</p>
<p>Kubeflow, TFX, MLflow κ°™μ€ λ„κµ¬ ν™μ©.</p>
<p>ν•µμ‹¬: λ¨λΈ κ°λ°λ¶€ν„° λ°°ν¬κΉμ§€μ λ°λ³µμ„ μλ™ν™”.</p>
<p>π”Ή Level 2: μλ™ λ¨λ‹ν„°λ§ λ° μ¬ν•™μµ (Continuous Training / MLOps)</p>
<p>μ΄μ μ¤‘ λ¨λΈμ„ μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§.</p>
<p>μ„±λ¥ μ§€ν‘, λ°μ΄ν„° λ¶„ν¬, drift νƒμ§€.</p>
<p>μ΄μƒ λ°μƒ μ‹ μλ™μΌλ΅ μ¬ν•™μµ νμ΄ν”„λΌμΈ μ‹¤ν–‰.</p>
<p>λ°μ΄ν„° λ“λ¦¬ν”„νΈΒ·κ°λ… λ“λ¦¬ν”„νΈ λ€μ‘ κ°€λ¥.</p>
<p>λ¨λΈ λ μ§€μ¤νΈλ¦¬μ™€ λ²„μ „ κ΄€λ¦¬ μ²΄κ³„ ν¬ν•¨.</p>
<p>π”Ή Level 3: μ™„μ „ μλ™ν™”λ ML μ‹μ¤ν… (Full MLOps / AutoMLOps)</p>
<p>λ°μ΄ν„° μμ§‘ β†’ ν•™μµ β†’ λ°°ν¬ β†’ λ¨λ‹ν„°λ§ β†’ μ¬ν•™μµκΉμ§€ μ—”λ“-ν¬-μ—”λ“ μλ™ν™”.</p>
<p>μΈμ  κ°μ… μµμ†ν™”, μ§€μ†μ μΈ λ¨λΈ κ°μ„ .</p>
<p>AutoML κΈ°λ²•κ³Ό κ²°ν•©λμ–΄, μƒλ΅μ΄ νƒμ¤ν¬μ— μλ™μΌλ΅ μ μ‘ κ°€λ¥.</p>
<p>λ€κ·λ¨ κΈ°μ—… ν™κ²½μ—μ„ λ©ν‘λ΅ μ‚Όλ” μµμΆ… λ‹¨κ³„.</p>
<p>π“ μ •λ¦¬:</p>
<p>Level 0 = μλ™, ad-hoc</p>
<p>Level 1 = ν•™μµ/λ°°ν¬ μλ™ν™” (νμ΄ν”„λΌμΈν™”)</p>
<p>Level 2 = λ¨λ‹ν„°λ§Β·μ¬ν•™μµ μλ™ν™” (μ§€μ†μ  μ΄μ)</p>
<p>Level 3 = μ™„μ „ μλ™ν™”, AutoMLκ³Ό κ²°ν•©</p>
<p><a href="https://velog.io/@leesjpr/MLOps-%EC%88%98%EC%A4%80">μ°Έκ³ </a></p>
